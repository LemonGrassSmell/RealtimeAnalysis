select MIN(beginTime) as min_t, MAX(begintime), file FROM parquet GROUP BY file ORDER BY min_t LIMIT 800;


====================================================== Time-fiber based selection rate ================================================
time range: 2016-08-10 10:14:37.105 | 2016-08-10 11:48:36.088    approx:100min
======================== Parquet ========================
MetaSQL: 
1%:  SELECT count(DISTINCT file) FROM queryParquet WHERE begintime < '2016-08-10 10:15:37';   [15 blocks]
2%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:16:37';   [15 blocks]
3%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:17:37';   [15 blocks]
4%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:18:37';   [54 blocks]
5%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:19:37';   [60 blocks]
10%: SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:24:37';   [88 blocks]

1%:  SELECT count(DISTINCT file) FROM queryParquet WHERE begintime < '2016-08-10 10:15:37' and partition=10;   [1 blocks]
2%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:16:37' and partition=10;   [1 blocks]
3%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:17:37' and partition=10;   [1 blocks]
4%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:18:37' and partition=10;   [2 blocks]
5%:  SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:19:37' and partition=10;   [4 blocks]
10%: SELECT count(DISTINCT file) FROM queryParquet WHERE beginTime < '2016-08-10 10:24:37' and partition=10;   [11 blocks]

QeurySQL:
SELECT sum(quantity) AS sum_qty, sum(extendedprice) AS sum_base_price, avg(quantity) AS avg_qty, avg(extendedprice) AS avg_price, avg(discount) AS avg_disc, count(*) AS count_order, min(orderkey) AS min_orderkey, max(orderkey) AS max_orderkey from realtime where messagedate < '2016-08-10 10:16' [and custkey = 1348000]

======================== Text ========================
2016-08-10 10:14:37.105 | 2016-08-10 11:20:09.34   approx:66min
1%:  SELECT count(DISTINCT file) FROM querytext WHERE begintime < '2016-08-10 10:15:37';   [15 blocks]
2%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:16:37';   [15 blocks]
3%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:17:37';   [15 blocks]
4%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:18:03';   [49 blocks]
5%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:18:04';   [62 blocks]
10%: SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:18:37';   [85 blocks]

1%:  SELECT count(DISTINCT file) FROM querytext WHERE begintime < '2016-08-10 10:15:37' and partition=10;   [1 blocks]
2%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:16:37' and partition=10;   [1 blocks]
3%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:17:37' and partition=10;   [1 blocks]
4%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:18:03' and partition=10;   [2 blocks]
5%:  SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:19:00' and partition=10;   [5 blocks]
10%: SELECT count(DISTINCT file) FROM querytext WHERE beginTime < '2016-08-10 10:21:00' and partition=10;   [10 blocks]

========================================================= Block based selection rate ================================================
======================== Parquet ========================
MetaSQL: 

5%:  SELECT DISTINCT file FROM metatable WHERE beginTime >= '2016-08-04 05:27:03.178' AND beginTime <= '2016-08-04 05:43:47.857';
10%: SELECT DISTINCT file FROM metatable WHERE beginTime >= '2016-08-04 05:27:03.178' AND beginTime <= '2016-08-04 06:05:14.3';
20%: SELECT DISTINCT file FROM metatable WHERE beginTime >= '2016-08-04 05:27:03.178' AND beginTime <= '2016-08-04 06:25:04.66';
40%: SELECT DISTINCT file FROM metatable WHERE beginTime >= '2016-08-04 05:27:03.178' AND beginTime <= '2016-08-09 03:54:08.462';
60%: SELECT DISTINCT file FROM metatable WHERE beginTime >= '2016-08-04 05:27:03.178' AND beginTime <= '2016-08-09 04:38:50.361';
80%: SELECT DISTINCT file FROM metatable WHERE beginTime >= '2016-08-04 05:27:03.178' AND beginTime <= '2016-08-09 05:18:14.262';
100%:SELECT DISTINCT file FROM metatable;

QuerySQL: 
SELECT sum(quantity) AS sum_qty, sum(extendedprice) AS sum_base_price, avg(quantity) AS avg_qty, avg(extendedprice) AS avg_price, avg(discount) AS avg_disc, count(*) AS count_order, min(orderkey) AS min_orderkey, max(orderkey) AS max_orderkey from realtime where messagedate >= '2016-08-04 05:27:03.178' and messagedate <= '2016-08-04 05:43:47.857' GROUP BY returnflag, linestatus ORDER BY returnflag, linestatus

======================== Text ========================
MetaSQL: 

5%:  SELECT DISTINCT file FROM text WHERE beginTime >= '2016-08-09 20:40:15.443' AND beginTime <= '2016-08-09 20:43:24.246';
10%: SELECT DISTINCT file FROM text WHERE beginTime >= '2016-08-09 20:40:15.443' AND beginTime <= '2016-08-09 20:46:38.842';
20%: SELECT count(DISTINCT file) FROM text WHERE beginTime >= '2016-08-09 20:40:15.443' AND beginTime <= '2016-08-09 21:51:29.364';
40%: SELECT count(DISTINCT file) FROM text WHERE beginTime >= '2016-08-09 20:40:15.443' AND beginTime <= '2016-08-09 21:56:08.67';
60%: SELECT count(DISTINCT file) FROM text WHERE beginTime >= '2016-08-09 20:40:15.443' AND beginTime <= '2016-08-09 22:00:45.194';
80%: SELECT count(DISTINCT file) FROM text WHERE beginTime >= '2016-08-09 20:40:15.443' AND beginTime <= '2016-08-09 22:05:23.821';
100%:SELECT DISTINCT file FROM text;

QuerySQL: 
SELECT sum(quantity) AS sum_qty, sum(extendedprice) AS sum_base_price, avg(quantity) AS avg_qty, avg(extendedprice) AS avg_price, avg(discount) AS avg_disc, count(*) AS count_order, min(orderkey) AS min_orderkey, max(orderkey) AS max_orderkey from realtime where messagedate >= '2016-08-09 20:40:15.443' and messagedate <= '2016-08-09 20:43:24.246' and custkey='569326' GROUP BY returnflag, linestatus ORDER BY returnflag, linestatus

import spark.implicits._
val df =spark.read.parquet("hdfs://192.168.7.33:9000/parquet/0146988184439814698818450381.305185422185319E12", "hdfs://192.168.7.33:9000/parquet/0146988184371514698818443991.2309846583839316E12", "hdfs://192.168.7.33:9000/new/0146988184371514698818443991.3696870208825889E12", "hdfs://192.168.7.33:9000/new/0146988184307414698818437131.7733335343426409E12", "hdfs://192.168.7.33:9000/parquet/0146988184307414698818437135.252003495172244E11", "hdfs://192.168.7.33:9000/parquet/0146988184243414698818430601.2472356966790952E12", "hdfs://192.168.7.33:9000/new/0146988184243414698818430601.4499423106307068E12", "hdfs://192.168.7.33:9000/new/0146988184173914698818424351.706477880246702E12", "hdfs://192.168.7.33:9000/parquet/0146988184173914698818424358.409865992580405E11", "hdfs://192.168.7.33:9000/parquet/0146988184106314698818417397.626881147098783E11"
, "hdfs://192.168.7.33:9000/new/0146988184106314698818417391.4259622619805044E12")
df.createOrReplaceTempView("realtime")
val resultDF = spark.sql("select sum(quantity) as sum_qty, sum(extendedprice) as sum_base_price, avg(quantity) as avg_qty, avg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order from realtime")
resultDF.show()
df.count()

val resultDF = spark.sql("select returnflag, linestatus, sum(quantity) as sum_qty, sum(extendedprice) as sum_base_price, sum(extendedprice*(1−discount)) as sum_disc_price, sum(extendedprice*(1−discount)*(1+tax)) as sum_charge , avg(quantity) as avg_qty, avg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order from realtime group by returnflag, linestatus order by returnflag, linestatus")
endTimeQuery = time.clock()
runTimeQuery = endTimeQuery - startTimeQuery
=======================================================================================================
val df = spark.read.parquet("hdfs://192.168.7.33:9000/parquet/0146988185770714698818583471.8012813886464585E12", "hdfs://192.168.7.33:9000/parquet/0146988185703814698818576891.7977804903634304E12", "hdfs://192.168.7.33:9000/parquet/0146988185634014698818570351.3676772448397246E12", "hdfs://192.168.7.33:9000/parquet/0146988185567114698818563385.983907080289454E11", "hdfs://192.168.7.33:9000/parquet/0146988185498114698818556741.9985738255683728E12")




 "hdfs://192.168.7.33:9000/parquet/0146988184770214698818483682.255992968808366E12", "hdfs://192.168.7.33:9000/new/0146988184702314698818476982.2303935642712335E11", "hdfs://192.168.7.33:9000/parquet/0146988184702314698818476986.508860729445415E11", "hdfs://192.168.7.33:9000/new/0146988184637614698818470191.804064019900904E12", "hdfs://192.168.7.33:9000/parquet/0146988184637614698818470198.337868612416213E11"
 
 "hdfs://192.168.7.33:9000/new/0146988184572214698818463731.5170068605864653E12", "hdfs://192.168.7.33:9000/parquet/0146988184572214698818463732.3705281857217646E12", "hdfs://192.168.7.33:9000/new/0146988184504314698818457142.471744920897673E12", "hdfs://192.168.7.33:9000/parquet/0146988184504314698818457141.2284601086563008E12", "hdfs://192.168.7.33:9000/new/0146988184439814698818450381.0805672128046439E12"
 
 
 
 hdfs://192.168.7.33:9000/parquet/0146988184036814698818410576.716083420584012E10
 hdfs://192.168.7.33:9000/new/0146988184036814698818410573.6338945535834607E11
 hdfs://192.168.7.33:9000/new/0146988183972414698818403676.287977475562339E11
 hdfs://192.168.7.33:9000/parquet/0146988183972414698818403673.062843247256443E11
 hdfs://192.168.7.33:9000/new/0146988183904614698818397191.4037303644334332E11
 hdfs://192.168.7.33:9000/parquet/0146988183904614698818397191.4983427789164836E12
 hdfs://192.168.7.33:9000/new/0146988183840514698818390301.4446727860935477E11
 hdfs://192.168.7.33:9000/parquet/0146988183840514698818390304.546912892264241E10
 hdfs://192.168.7.33:9000/new/0146988183776114698818383881.8747168941527642E12
 hdfs://192.168.7.33:9000/parquet/0146988183776114698818383883.944204109363376E10
 hdfs://192.168.7.33:9000/new/0146988183706214698818377562.0190036090671543E12
 hdfs://192.168.7.33:9000/parquet/0146988183706214698818377567.765221659322577E11
 hdfs://192.168.7.33:9000/parquet/0146988183641214698818370546.658460479934479E11
 hdfs://192.168.7.33:9000/new/0146988183641214698818370541.2020334546925667E12
 hdfs://192.168.7.33:9000/parquet/0146988183574314698818363943.6631770041355725E11
 hdfs://192.168.7.33:9000/new/0146988183574314698818363946.736743794058804E11
 hdfs://192.168.7.33:9000/new/0146988183509314698818357444.5822892540053204E11
 hdfs://192.168.7.33:9000/parquet/0146988183509314698818357447.962838712277145E9
 hdfs://192.168.7.33:9000/parquet/0146988183441214698818350937.839163206233387E11
 hdfs://192.168.7.33:9000/new/0146988183441214698818350939.776227971811737E11
 hdfs://192.168.7.33:9000/new/0146988183375614698818344041.0327675279864022E12
 hdfs://192.168.7.33:9000/parquet/0146988183375614698818344042.1980192612233223E10
 hdfs://192.168.7.33:9000/parquet/0146988183307514698818337561.9918480122071367E12
 hdfs://192.168.7.33:9000/new/0146988183307514698818337561.626820525245435E12

 hdfs://192.168.7.33:9000/parquet/0146988183241614698818330571.1342835924120466E12
 hdfs://192.168.7.33:9000/new/0146988183241614698818330576.4771329652358635E10
 hdfs://192.168.7.33:9000/new/0146988183176314698818324126.657152176639874E11
 hdfs://192.168.7.33:9000/parquet/0146988183176314698818324121.5163651989803374E12
 hdfs://192.168.7.33:9000/parquet/0146988183108814698818317541.4175048224427021E12
 hdfs://192.168.7.33:9000/new/0146988183108814698818317541.0092599286439719E9
 hdfs://192.168.7.33:9000/new/0146988183044514698818310731.305788570245205E12
 hdfs://192.168.7.33:9000/parquet/0146988183044514698818310732.0884655396325005E12
 hdfs://192.168.7.33:9000/parquet/0146988182976814698818304381.0111791308977015E12
 hdfs://192.168.7.33:9000/new/0146988182976814698818304388.927866318694988E11

 ===================== ===================== ===================== ===================== ===================== =====================
import spark.implicits._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

case class realtime(custkey: Integer, orderkey: Integer, partkey: Integer, suppkey: Integer, linenumber: Integer, quantity: Float, extendedprice: Float, discount: Float, tax: Float, returnflag: String, linestatus: String, shipdate: String, commitdate: String, receiptdate: String, shipstruct: String, shipmode: String, comment: String, orderstatus: String, totalprice: Float, orderdate: String, orderpriority: String, clerk: String, shippriority: Integer, ordercomment: String, messagedate: String)
val rt = spark.sparkContext.textFile("hdfs://192.168.7.33:9000/text/01*").map(_.split("\\|")).map(attrs => realtime(attrs(0).trim.toInt, attrs(1).trim.toInt, attrs(2).trim.toInt, attrs(3).trim.toInt, attrs(4).trim.toInt, attrs(5).trim.toFloat, attrs(6).trim.toFloat, attrs(7).trim.toFloat, attrs(8).trim.toFloat, attrs(9), attrs(10),attrs(11),attrs(12),attrs(13),attrs(14),attrs(15),attrs(16),attrs(17), attrs(18).trim.toFloat,attrs(19),attrs(20),attrs(21),attrs(22).trim.toInt,attrs(23), attrs(24))).toDF()

rt.createOrReplaceTempView("rt")
val resultDF = spark.sql("select sum(quantity) as sum_qty, sum(extendedprice) as sum_base_price, avg(quantity) as avg_qty, avg(extendedprice) as avg_price, avg(discount) as avg_disc, count(*) as count_order from rt")
resultDF.show()
rt.count()

val test = spark.sparkContext.textFile("hdfs://192.168.7.33:9000/text/0146981744583814698174654782.0194399064028755E12", "hdfs://192.168.7.33:9000/text/0146981746547814698174778462.1719210430573867E12").map(_.split("|")).map(attrs => realtime(attrs(0).trim.toInt, attrs(1).trim.toInt, attrs(2).trim.toInt, attrs(3).trim.toInt, attrs(4).trim.toInt, attrs(5).trim.toFloat, attrs(6).trim.toFloat, attrs(7).trim.toFloat, attrs(8).trim.toFloat, attrs(9), attrs(10),attrs(11),attrs(12),attrs(13),attrs(14),attrs(15),attrs(16),attrs(17).trim.toFloat,attrs(18),attrs(19),attrs(20),attrs(21).trim.toInt,attrs(22),attrs(23))).toDF()

test.createOrReplaceTempView("test")

val res = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")

 
 
 "hdfs://192.168.7.33:9000/test/1146981746549014698174780461.606271978016163E12"
 
 
 
 
 
 
 "hdfs://192.168.7.33:9000/text/0146981744583814698174654782.0194399064028755E12", "hdfs://192.168.7.33:9000/text/0146981746547814698174778462.1719210430573867E12", "hdfs://192.168.7.33:9000/text/0146981747784514698174897042.2884579484978818E11", "hdfs://192.168.7.33:9000/text/0146981748970414698175014871.8697489443576948E12", "hdfs://192.168.7.33:9000/text/0146981750148814698175119581.4628860066413203E12", "hdfs://192.168.7.33:9000/text/0146981751195614698175226671.1930735781219041E11", "hdfs://192.168.7.33:9000/text/0146981752266714698175334881.3482771876010173E12", "hdfs://192.168.7.33:9000/text/0146981753348814698175442591.7202165659849836E12", "hdfs://192.168.7.33:9000/text/0146981754425914698175548643.58029364078696E11", "hdfs://192.168.7.33:9000/text/0146981755486414698175655752.3648035999286025E12", "hdfs://192.168.7.33:9000/test/1146981744583714698174654901.2577656514674976E11"
 
 hdfs://192.168.7.33:9000/test/1146981746549014698174780461.606271978016163E12
 hdfs://192.168.7.33:9000/test/1146981747804614698174899231.171242144889171E12
 hdfs://192.168.7.33:9000/test/1146981748992214698175015545.804345107922341E11
 hdfs://192.168.7.33:9000/test/1146981750155414698175122627.196928498171877E11
 hdfs://192.168.7.33:9000/test/1146981751226214698175229171.2226220741401362E12